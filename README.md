# BIO-Index

BIO-Index is a tool that indexes genomic data stored in [AWS S3][s3] "tables" (typically generated by [Spark][spark]) so that it can be rapidly queried and loaded. It uses a [Redis][redis] database to store the indexes and to look up where in [S3][s3] each "record" is located.

The BIO-Index has two entry points: a CLI used for basic CRUD operations and a Flask server REST API for pure querying. 

## Quickstart

A quick guide on getting the BIO-Index built, querying it, and serving a REST API with it.

### Redis

To use the BIO-Index, you first need to have a [Redis][redis] database at your disposal. The easiest way to do this is with [Docker][docker], exposing the port, and mounting a volume for persistence. Example (when run from an EC2 instance):

```bash
$ docker run -v /home/ec2-user/bio-index-db/:/data -p 6379:6379 --name bioindex -d redis
```

This will start a new database running, saving it to `/home/ec2-user/bio-index-db` and exposing it on port `6379` (the default port).

### Dot Environment

The bio-index uses [python-dotenv][dotenv] (environment variables) for configuration. There are two environment files of importance: `.env` and `.flaskenv`.

The `.env` file contains the settings for connecting to the [Redis][redis] database and which [AWS S3][s3] bucket to use for table storage. It can also be used to override AWS credentials if desired.

The `.flaskenv` file holds any Flask, server-specific settings (e.g. app module and port number).

_Q: Why is the [S3][s3] bucket an environment parameter?_

_A: Because you may want to sync buckets and use the same index. For example, you build the index using one bucket, sync it to a production bucket, and then can update the original bucket without breaking the indices._

### Testing Setup

Once you think you have everything setup, you can test it with the CLI:

```bash
$ python3 main.py test
```

### Preparing Tables

Once everything is setup, you can begin creating or preparing the "table" files in [S3][s3] to be indexed. Each table is expected to be in a [JSON-lines][json-lines] format. This is output natively by [Spark][spark] to folders in [S3][s3] when used to process data. For example (using [PySpark][pyspark] on an [AWS EMR][emr] cluster):

```python
df.write. \
    mode('overwrite'). \
    json('s3://my-bucket/path/to/output')
```

The above code would write out many part files to the bucket/path that can now be indexed using the `index` CLI command. However, they will not be well suited for high-performance reading. It is best to always order the output files by locus before writing them. This will _dramatically_ improve the performance of BIO-Index:

```python
sorted_snp_df = df.orderBy(['chromosome', 'pos'])
sorted_range_df = df.orderBy(['chromosome', 'start'])
```

### Indexing Tables

All records in the BIO-Index are indexed by locus. For SNPs, this will be chromosome + position, while for regions this will be a chromosome, start, and stop. Additionally, each table is considered to be of a single data type and is indexed into a single key space within [Redis][redis].

For example, let's say you have a file stored in [S3][s3] of SNPs, where each record in the file looked like this:

```json
{"chr":"8","pos":48348712,"ref":"T","alt":"GAA","pValue":0.04128}
```

You could index it like so:

```bash
$ python3 main.py index snps path/to/snp/file.json chr:pos
```

The above will index every record in `path/to/snp/file.json` into the [Redis][redis] key space `snps` using the locus defined by `chr:pos`.

_Note: multiple tables can all be indexed into the same Redis key space!_

If you had a table of region data where each record looks like this:

```json
{"method":"ChromHMM","annotation":"Promoter","chromosome":"X","start":83282,"end":83499,"score":34}
```

That table might be indexed like so:

```bash
$ python3 main.py index annotations path/to/region/file.json chromosome:start-end
```

_Notice how the locus parameter on the CLI identified 3 fields in the JSON instead of just 2._

It is also possible to recursively index all files in an entire folder of [S3][s3] by adding `/` to the end of the path.

```bash
$ python3 main.py index genes path/to/genes/ chrom:start-stop
```

### Querying Records

Once you've built an index, you can then query the key space and retrieve all the records that overlap a given locus. For example, to query all records in the `genes` key space that overlap a given region:

```bash
$ python3 main.py query genes chr8:100000-101000
{"name": "AC131281.2", "chromosome": "8", "start": 100584, "end": 100728, "ensemblId": "ENSG00000254193", "type": "processed_pseudogene"}
```

The input region is always in the format `chromosome:start[-stop]`. The `chromosome` may optionally be prefixed with "chr", but doesn't need to be. If the `stop` is not provided, it defaults to `start+1`.

_Note: the input locus is in the range `[start,stop)`._

### Checking and Updating Indexes

Data changes. That's life. And, if you use a process like [Spark][spark] to generate your data, it will blow away existing tables that were previously indexed, orphaning many records in your indexes.

#### Checking + Deleting Tables

To check the tables in the database, you can use the `check` command. This will verify that every table indexed still exists in [S3][s3].

```bash
$ python3 main.py check
INFO  - Running table check...
INFO  - Check complete
```

Any tables that are no longer present will output a warning. If you pass `--delete` on the command line as well, then any tables that no longer exist will be removed from the key space they were added to along with all records that pointed to them.

#### Updating Tables

If you have a table (or path of tables) that occupy the same space in [S3][s3] (i.e. have the same key), then you can just use the `index` command along with the `--update` flag. Any tables being indexed that already exist will have all their records deleted from the key space and then be re-indexed.

_Note: when updating tables, you still need to provide a "locus" string that identifies the JSON keys to use for indexing. BIO-Index does not assume they are the same as they were previously!_

#### Indexing New Tables

Perhaps you have an entire folder in [S3][s3] with tables that have already been indexed, and have now just added a couple additional ones you'd also like indexed. To do this, use the `index` command with the `--new` flag. And tables that have already been indexed will be skipped, and new tables will be indexed.

_Note: you can supply both the `--new` and `--update` flags, which will index new tables and update existing ones in the same pass._

## Index REST Server

In addition to a CLI, BIO-Index is also a [Flask][flask] server that allows you to query records via REST calls.

### Starting the Server

Create (or edit) the `.flaskenv` file to set the `FLASK_APP` to `server:app` and optionally the `FLASK_PORT` (default is 5000). Then run:

```bash
$ flask run
```

_Note: this assumes `flask` is installed via `pip`. This is also the development environment. If you wish to run this in production, follow the guides on the [Flask][flask] website for doing so._

### REST Queries

TODO:

### Dependencies

* [Python 3.6+][python]
* [python-dotenv][dotenv]
* [click][click]
* [flask][flask]
* [boto3][boto3]
* [redis][python-redis]
* [msgpack][msgpack]
* [smart_open][smart_open]

# fin.

[python]: https://www.python.org/
[dotenv]: https://saurabh-kumar.com/python-dotenv/
[redis]: https://redis.io/
[docker]: https://hub.docker.com/_/redis/
[s3]: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html
[emr]: https://aws.amazon.com/emr/
[click]: https://click.palletsprojects.com/en/7.x/quickstart/
[flask]: https://www.palletsprojects.com/p/flask/
[boto3]: https://aws.amazon.com/sdk-for-python/
[msgpack]: https://msgpack-python.readthedocs.io/en/latest/api.html
[smart_open]: https://pypi.org/project/smart-open/
[python-redis]: https://pypi.org/project/redis/
[spark]: https://spark.apache.org/
[pyspark]: https://spark.apache.org/docs/latest/api/python/pyspark.html
[json-lines]: http://jsonlines.org/examples/

# BIO-Index

BIO-Index is a tool that indexes genomic data stored in [AWS S3][s3] "tables" (typically generated by [Spark][spark]) so that it can be rapidly queried and loaded. It uses a [Redis][redis] database to store the indexes and to look up where in [S3][s3] each "record" is located.

The BIO-Index has two entry points: a CLI used for basic CRUD operations and a Flask server REST API for pure querying. 

## Quickstart

A quick guide on getting the BIO-Index built, querying it, and serving a REST API with it.

### Redis

To use the BIO-Index, you first need to have a [Redis][redis] database at your disposal. The easiest way to do this is with [Docker][docker], exposing the port, and mounting a volume for persistence. Example (when run from an EC2 instance):

```bash
$ docker run -v /home/ec2-user/bio-index-db/:/data -p 6379:6379 --name bioindex -d redis
```

This will start a new database running, saving it to `/home/ec2-user/bio-index-db` and exposing it on port `6379` (the default port).

### Dot Environment

The bio-index uses [python-dotenv][dotenv] (environment variables) for configuration. There are two environment files of importance: `.env` and `.flaskenv`.

The `.env` file contains the settings for connecting to the [Redis][redis] database and which [AWS S3][s3] bucket to use for table storage. It can also be used to override AWS credentials if desired.

The `.flaskenv` file holds any Flask, server-specific settings (e.g. app module and port number).

_Q: Why is the [S3][s3] bucket an environment parameter?_

_A: Because you may want to sync buckets and use the same index. For example, you build the index using one bucket, sync it to a production bucket, and then can update the original bucket without breaking the indices._

### Testing Setup

Once you think you have everything setup, you can test it with the CLI:

```bash
$ python3 main.py test
```

### Preparing Tables

Once everything is setup, you can begin creating or preparing the "table" files in [S3][s3] to be indexed. Each table is expected to be in a [JSON-lines][json-lines] format. This is output natively by [Spark][spark] to folders in [S3][s3] when used to process data. For example (using [PySpark][pyspark] on an [AWS EMR][emr] cluster):

```python
df.write. \
    mode('overwrite'). \
    json('s3://my-bucket/path/to/output')
```

The above code would write out many part files to the bucket/path that can now be indexed using the `index` CLI command. However, they will not be well suited for high-performance reading. It is best to always order the output files by locus before writing them. This will _dramatically_ improve the performance of BIO-Index:

```python
sorted_snp_df = df.orderBy(['chromosome', 'pos'])
sorted_range_df = df.orderBy(['chromosome', 'start'])
```

### Indexing Tables

All records in the BIO-Index are indexed by locus. For SNPs, this will be chromosome + position, while for regions this will be a chromosome, start, and stop. Additionally, each table is considered to be of a single data type and is indexed into a single key space within [Redis][redis].

For example, let's say you have a file stored in [S3][s3] of SNPs, where each record in the file looked like this:

```json
{"chr":"8","pos":48348712,"ref":"T","alt":"GAA","pValue":0.04128}
```

You could index it like so:

```bash
$ python3 main.py index snps path/to/snp/file.json chr:pos
```

The above will index every record in `path/to/snp/file.json` into the [Redis][redis] key space `snps` using the locus defined by `chr:pos`.

_Note: multiple tables can all be indexed into the same Redis key space!_

If you had a table of region data where each record looks like this:

```json
{"method":"ChromHMM","annotation":"Promoter","chromosome":"X","start":83282,"end":83499,"score":34}
```

That table might be indexed like so:

```bash
$ python3 main.py index annotations path/to/region/file.json chromosome:start-end
```

_Notice how the locus parameter on the CLI identified 3 fields in the JSON instead of just 2._

It is also possible to recursively index all files in an entire folder of [S3][s3] by adding `/` to the end of the path.

```bash
$ python3 main.py index genes path/to/genes/ chrom:start-stop
```

### Querying Records

Once you've built an index, you can then query the key space and retrieve all the records that overlap a given locus. For example:

```bash
$ python3 main.py query chr8:100000-101000
```

### Running the Server

Create (or edit) the `.flaskenv` file to set the `FLASK_APP` to `server:app` and optionally the `FLASK_PORT` (default is 5000).

### Dependencies

* [Python 3.6+][python]
* [python-dotenv][dotenv]
* [click][click]
* [flask][flask]
* [boto3][boto3]
* [redis][python-redis]
* [msgpack][msgpack]
* [smart_open][smart_open]

# fin.

[python]: https://www.python.org/
[dotenv]: https://saurabh-kumar.com/python-dotenv/
[redis]: https://redis.io/
[docker]: https://hub.docker.com/_/redis/
[s3]: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html
[emr]: https://aws.amazon.com/emr/
[click]: https://click.palletsprojects.com/en/7.x/quickstart/
[flask]: https://www.palletsprojects.com/p/flask/
[boto3]: https://aws.amazon.com/sdk-for-python/
[msgpack]: https://msgpack-python.readthedocs.io/en/latest/api.html
[smart_open]: https://pypi.org/project/smart-open/
[python-redis]: https://pypi.org/project/redis/
[spark]: https://spark.apache.org/
[pyspark]: https://spark.apache.org/docs/latest/api/python/pyspark.html
[json-lines]: http://jsonlines.org/examples/
